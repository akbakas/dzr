{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393c2278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f1115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "\n",
    "import albumentations as albu\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2802d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "DATA_DIR = \"/media/akhan/NVME 1TB/RZD/data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee1c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "        \n",
    "    def __init__(\n",
    "            self, \n",
    "            images: List[str],\n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        \n",
    "        self.basewidth = WIDTH\n",
    "        self.images_list = images\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image = Image.open(self.images_list[i])\n",
    "        wpercent = (self.basewidth/float(image.size[0]))\n",
    "        hsize = int((float(image.size[1])*float(wpercent)))\n",
    "        image = image.resize((self.basewidth, hsize), resample=Image.ANTIALIAS, reducing_gap=3.)\n",
    "        image = np.array(image)\n",
    "        image, remainder = pad(image)\n",
    "        \n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image)\n",
    "            image = sample['image']\n",
    "            \n",
    "        return {\"image\": image, \"remainder\": remainder}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16d495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_IMG = glob.glob(os.path.join(DATA_DIR, \"*.png\"))\n",
    "x_val = np.array(ALL_IMG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d244a958",
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8087d68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(img: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Pads an array so that it's height is divisible by 32,\n",
    "    given it's width is divisible by 32 already.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like\n",
    "        image (H, W, C)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (padded image (H+p, W, C), int)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    h, w = img.shape[:2]\n",
    "    assert w % 32 == 0, \"Image width must be divisible by 32\"\n",
    "    if h % 32 == 0:\n",
    "        return (img, 0)\n",
    "\n",
    "    remainder = 32 - (h + 32) % 32\n",
    "    img = np.pad(img, ((remainder, 0), (0, 0), (0, 0)))\n",
    "\n",
    "    return (img, remainder)\n",
    "\n",
    "def unpad(padded: np.array, remainder: int) -> np.array:\n",
    "\n",
    "    return padded[remainder:, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a149b18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d49469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rzdModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, arch, encoder_name, in_channels, out_classes, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = smp.create_model(\n",
    "            arch, encoder_name=encoder_name, in_channels=in_channels, classes=out_classes, **kwargs\n",
    "        )\n",
    "\n",
    "        # preprocessing parameteres for image\n",
    "        params = smp.encoders.get_preprocessing_params(encoder_name)\n",
    "        self.register_buffer(\"std\", torch.tensor(params[\"std\"]).view(1, 3, 1, 1))\n",
    "        self.register_buffer(\"mean\", torch.tensor(params[\"mean\"]).view(1, 3, 1, 1))\n",
    "\n",
    "        # for image segmentation dice loss could be the best first choice\n",
    "        self.loss_fn = smp.losses.DiceLoss(smp.losses.MULTILABEL_MODE, smooth=0.05)\n",
    "#         self.loss_fn = smp.losses.TverskyLoss(smp.losses.BINARY_MODE, alpha=0.6, beta=0.4, gamma=1.4)\n",
    "    \n",
    "        self.freeze_encoder()\n",
    "        \n",
    "    def forward(self, image):\n",
    "        # normalize image here\n",
    "#         image = (image - self.mean) / self.std\n",
    "        mask = self.model(image)\n",
    "        return mask\n",
    "\n",
    "    def freeze_encoder(self):\n",
    "        for child in self.model.encoder.children():\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "        return\n",
    "    \n",
    "    def shared_step(self, batch, stage):\n",
    "        \n",
    "        image = batch[\"image\"]\n",
    "\n",
    "        # Shape of the image should be (batch_size, num_channels, height, width)\n",
    "        # if you work with grayscale images, expand channels dim to have [batch_size, 1, height, width]\n",
    "        assert image.ndim == 4\n",
    "\n",
    "        # Check that image dimensions are divisible by 32, \n",
    "        # encoder and decoder connected by `skip connections` and usually encoder have 5 stages of \n",
    "        # downsampling by factor 2 (2 ^ 5 = 32); e.g. if we have image with shape 65x65 we will have \n",
    "        # following shapes of features in encoder and decoder: 84, 42, 21, 10, 5 -> 5, 10, 20, 40, 80\n",
    "        # and we will get an error trying to concat these features\n",
    "        h, w = image.shape[2:]\n",
    "        assert h % 32 == 0 and w % 32 == 0\n",
    "\n",
    "        mask = batch[\"mask\"]\n",
    "        # Shape of the mask should be [batch_size, num_classes, height, width]\n",
    "        # for binary segmentation num_classes = 1\n",
    "        assert mask.ndim == 4\n",
    "\n",
    "        # Check that mask values in between 0 and 1, NOT 0 and 255 for binary segmentation\n",
    "#         assert mask.max() <= 1.0 and mask.min() >= 0\n",
    "\n",
    "        logits_mask = self.forward(image)\n",
    "        \n",
    "        # Predicted mask contains logits, and loss_fn param `from_logits` is set to True\n",
    "        loss = self.loss_fn(logits_mask, mask)\n",
    "\n",
    "        # Lets compute metrics for some threshold\n",
    "        # first convert mask values to probabilities, then \n",
    "        # apply thresholding\n",
    "        prob_mask = logits_mask.sigmoid()\n",
    "        pred_mask = (prob_mask > 0.5).float()\n",
    "\n",
    "        # We will compute IoU metric by two ways\n",
    "        #   1. dataset-wise\n",
    "        #   2. image-wise\n",
    "        # but for now we just compute true positive, false positive, false negative and\n",
    "        # true negative 'pixels' for each image and class\n",
    "        # these values will be aggregated in the end of an epoch\n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), mask.long(), mode=\"multilabel\")\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"tn\": tn,\n",
    "        }\n",
    "\n",
    "    def shared_epoch_end(self, outputs, stage):\n",
    "        # aggregate step metics\n",
    "        tp = torch.cat([x[\"tp\"] for x in outputs])\n",
    "        fp = torch.cat([x[\"fp\"] for x in outputs])\n",
    "        fn = torch.cat([x[\"fn\"] for x in outputs])\n",
    "        tn = torch.cat([x[\"tn\"] for x in outputs])\n",
    "\n",
    "        # per image IoU means that we first calculate IoU score for each image \n",
    "        # and then compute mean over these scores\n",
    "        per_image_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n",
    "        \n",
    "        # dataset IoU means that we aggregate intersection and union over whole dataset\n",
    "        # and then compute IoU score. The difference between dataset_iou and per_image_iou scores\n",
    "        # in this particular case will not be much, however for dataset \n",
    "        # with \"empty\" images (images without target class) a large gap could be observed. \n",
    "        # Empty images influence a lot on per_image_iou and much less on dataset_iou.\n",
    "        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "\n",
    "        metrics = {\n",
    "            f\"{stage}_per_image_iou\": per_image_iou,\n",
    "            f\"{stage}_dataset_iou\": dataset_iou,\n",
    "        }\n",
    "        \n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"train\")            \n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        return self.shared_epoch_end(outputs, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"valid\")\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        return self.shared_epoch_end(outputs, \"valid\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"test\")  \n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        return self.shared_epoch_end(outputs, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d5ff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCH = \"UnetPlusPlus\"\n",
    "ENCODER = \"resnext50_32x4d\"\n",
    "ENCODER_WEIGHTS = \"imagenet\"\n",
    "NUM_CLASS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74842d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "model = rzdModel(ARCH, ENCODER, in_channels=3, out_classes=NUM_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b720230",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"../dzr/ckpts/multilabel_1024/resnext50_32x4d/epoch=59-step=110760.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bc968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981dbc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "    \n",
    "\n",
    "encoder_params = smp.encoders.get_preprocessing_params(ENCODER)\n",
    "std = encoder_params['std']\n",
    "mean = encoder_params['mean']\n",
    "\n",
    "postprocess_fn = UnNormalize(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07942c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Dataset(x_val,\n",
    "                      preprocessing=get_preprocessing(preprocessing_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b27774",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3ce076",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "for batch in val_loader:\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        logits = model(batch[\"image\"].to(device))\n",
    "        remainder  = int(batch[\"remainder\"])\n",
    "    pr_masks = logits.sigmoid()\n",
    "    pr_masks = (pr_masks > 0.5).float()\n",
    "    for pr_mask in pr_masks:\n",
    "        pr_mask = pr_mask.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "        pr_mask = unpad(pr_mask, remainder)\n",
    "        plt.figure(figsize=(16, 16))\n",
    "        \n",
    "        # denormalize image\n",
    "        image = batch['image']\n",
    "        image = postprocess_fn(image) * 255\n",
    "        image = image.squeeze().permute(1,2,0).numpy().astype(np.uint8)\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(image)  # convert CHW -> HWC\n",
    "        plt.title(\"Image\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(pr_mask[..., 2]) # just squeeze classes dim, because we have only one class\n",
    "        plt.title(\"Prediction\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc750fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gedit .gitignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fe430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git status"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Segmentation",
   "language": "python",
   "name": "segmentation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
